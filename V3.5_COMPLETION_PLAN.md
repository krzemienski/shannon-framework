# Shannon V3.5 - REAL Completion Plan

**Reality Check**: V3.5 is currently 40% complete (not 80%)  
**Problem**: All execution is stubbed - nothing actually runs  
**Solution**: Remove ALL stubs, implement REAL execution, test end-to-end  
**Timeline**: 7-10 hours of focused work across 4 waves

---

## Current State (Honest Assessment)

### What Works ✅
- PromptEnhancer (100%) - Builds real enhanced prompts
- Project detection (100%) - Detects real project types
- Data models (100%) - Serialize correctly
- Algorithms (100%) - Branch naming, scoring logic

### What's Stubbed ❌
- Library search: Returns `[]` instead of real results
- Validation execution: Returns `True` instead of running tests
- Git execution: Returns `""` instead of running git
- MCP integration: Code exists but not wired up

### What's Missing ❌
- End-to-end orchestration
- Real command execution
- Real MCP calls
- Real git operations
- Usable exec command

---

## Completion Waves with Validation Gates

### Wave 1: Real Command Execution (2-3 hours)

**Goal**: Remove ALL stubs, implement REAL execution

#### Agent A: ValidationOrchestrator - Real Test Execution

**Task**: Make validation actually run tests

**Files**: `src/shannon/executor/validator.py`

**Work**:
```python
async def _run_check(self, command: str, check_name: str) -> bool:
    # OLD (stub):
    # return True
    
    # NEW (real):
    import subprocess
    try:
        result = subprocess.run(
            command,
            shell=True,
            cwd=self.project_root,
            capture_output=True,
            text=True,
            timeout=300  # 5 min timeout
        )
        return result.returncode == 0
    except subprocess.TimeoutExpired:
        self.logger.error(f"{check_name} timed out")
        return False
    except Exception as e:
        self.logger.error(f"{check_name} failed: {e}")
        return False
```

**Lines**: ~50  
**Time**: 45 minutes  
**Validation**: Run on this project, verify pytest actually executes

#### Agent B: GitManager - Real Git Operations

**Task**: Make git commands actually execute

**Files**: `src/shannon/executor/git_manager.py`

**Work**:
```python
async def _run_git(self, command: str) -> str:
    # OLD (stub):
    # return ""
    
    # NEW (real):
    import subprocess
    try:
        result = subprocess.run(
            f'git {command}',
            shell=True,
            cwd=self.project_root,
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode != 0:
            raise Exception(f"Git command failed: {result.stderr}")
        return result.stdout
    except Exception as e:
        self.logger.error(f"Git command failed: {e}")
        raise
```

**Lines**: ~40  
**Time**: 30 minutes  
**Validation**: Create real test branch, verify in `git branch`

#### Agent C: Dead Code Removal

**Task**: Remove any unused code

**Files**: All modules

**Work**:
- Review all modules
- Remove unused imports
- Remove commented code
- Remove TODOs that won't be done

**Lines**: -50 (deletions)  
**Time**: 30 minutes  
**Validation**: No linter warnings, clean code

**Wave 1 Validation Gate**:
```bash
# Must pass ALL:
python test_real_validation.py  # Runs real pytest
python test_real_git.py         # Creates real branch
python test_clean_code.py       # No dead code
```

---

### Wave 2: Real Library Discovery (2-3 hours)

#### Agent A: Web Search Integration

**Task**: Implement real library search using web_search

**Files**: `src/shannon/executor/library_discoverer.py`

**Work**:
```python
async def _search_npm(self, feature: str) -> List[Dict]:
    # Use web_search tool (available in this environment)
    query = f"npm {feature} package site:npmjs.com"
    
    # Import web_search from tools
    from tools import web_search  # Or however it's available
    
    results = web_search(query)
    
    # Parse results
    libraries = []
    for result in results:
        if 'npmjs.com/package/' in result.url:
            # Extract package name from URL
            pkg_name = result.url.split('/package/')[-1]
            
            # Fetch metadata (stars, downloads from result text)
            libraries.append(self._parse_npm_result(pkg_name, result))
    
    return libraries
```

**Lines**: ~150  
**Time**: 1.5 hours  
**Validation**: Search "react authentication", get real next-auth, passport results

#### Agent B: Result Parsing

**Task**: Parse search results into LibraryRecommendation

**Files**: `src/shannon/executor/library_discoverer.py`

**Work**:
- Parse npm search results
- Parse PyPI search results
- Parse GitHub search results
- Extract: name, description, stars, last_updated

**Lines**: ~100  
**Time**: 1 hour  
**Validation**: Parse real search results, verify all fields populated

#### Agent C: Serena MCP Caching

**Task**: Integrate Serena MCP for caching

**Files**: `src/shannon/executor/library_discoverer.py`

**Work**:
```python
async def _cache_in_serena(self, key: str, libraries: List[LibraryRecommendation]):
    try:
        # Use mcp_memory_create_entities
        entities = [{
            "name": f"library_{lib.name}",
            "entityType": "LibraryRecommendation",
            "observations": [
                f"Name: {lib.name}",
                f"Score: {lib.score}",
                f"Stars: {lib.stars}",
                f"Install: {lib.install_command}"
            ]
        } for lib in libraries]
        
        await mcp_memory_create_entities(entities=entities)
    except:
        pass  # Cache failure is non-fatal
```

**Lines**: ~80  
**Time**: 45 minutes  
**Validation**: Cache libraries, verify in Serena, load on next search

**Wave 2 Validation Gate**:
```bash
# Must pass ALL:
python test_real_library_search.py  # Finds real libraries
python test_serena_caching.py       # Caches and loads
python test_library_ranking.py      # Ranks correctly
```

---

### Wave 3: End-to-End Orchestration (2-3 hours)

#### Agent A: Simple Task Executor

**Task**: Create minimal working orchestrator

**Files**: `src/shannon/executor/simple_executor.py` (NEW, ~200 lines)

**Work**:
```python
class SimpleTaskExecutor:
    """
    Simple orchestrator that can execute tasks WITHOUT Shannon Framework skill
    
    Limited but functional:
    - Uses Claude SDK directly
    - Single-step execution
    - Real validation
    - Real git commits
    """
    
    async def execute_simple_task(self, task: str):
        # 1. Enhanced prompts
        prompts = PromptEnhancer().build_enhancements(task, cwd)
        
        # 2. Discover libraries
        libs = await LibraryDiscoverer(cwd).discover_for_feature(task)
        
        # 3. Build prompt with libraries
        final_prompt = f"{prompts}\n\nTask: {task}\n\nRecommended libraries: {libs}"
        
        # 4. Execute via Claude SDK
        from claude_agent_sdk import query
        messages = []
        async for msg in query(final_prompt):
            messages.append(msg)
        
        # 5. Extract changes from messages
        changes = self.extract_changes(messages)
        
        # 6. Validate
        validation = await ValidationOrchestrator(cwd).validate_all_tiers(changes)
        
        # 7. Commit if valid
        if validation.all_passed:
            await GitManager(cwd).commit_validated_changes(
                files=changes.files,
                step_description=task,
                validation_result=validation
            )
            return ExecutionResult(success=True, ...)
        else:
            return ExecutionResult(success=False, error="Validation failed")
```

**Lines**: ~200  
**Time**: 2 hours  
**Validation**: Execute real simple task, verify it works

#### Agent B: CLI Integration

**Task**: Wire simple executor to CLI

**Files**: `src/shannon/cli/commands.py`

**Work**:
- Replace preview mode with real SimpleTaskExecutor
- Show progress in V3.1 dashboard
- Handle errors properly

**Lines**: ~50 (modify existing)  
**Time**: 30 minutes  
**Validation**: Run `shannon exec "simple task"`, verify executes

#### Agent C: End-to-End Test

**Task**: Create comprehensive E2E test

**Files**: `test_v3.5_end_to_end.py` (NEW, ~150 lines)

**Work**:
- Test full workflow: task → prompts → libraries → execute → validate → commit
- Verify on real codebase
- Verify all modules used
- Verify produces real results

**Lines**: ~150  
**Time**: 1.5 hours  
**Validation**: E2E test passes

**Wave 3 Validation Gate**:
```bash
# Must pass:
python test_v3.5_end_to_end.py  # Complete workflow works
shannon exec "add comment to README.md"  # Actually works
git log  # Shows real commit
```

---

### Wave 4: Production Readiness (1-2 hours)

#### Single Agent: Final Polish

**Tasks**:
1. Remove all "PREVIEW" and "TODO" markers
2. Update documentation with what ACTUALLY works
3. Add error handling for edge cases
4. Create honest README for V3.5
5. Final testing on 3 different project types

**Lines**: ~100 modifications  
**Time**: 1-2 hours  
**Validation**: Can execute tasks on Python, React, and iOS projects

---

## Validation Gates Summary

**Gate 1** (after Wave 1):
- ✅ pytest actually runs and returns real results
- ✅ git commands actually execute
- ✅ No dead code

**Gate 2** (after Wave 2):
- ✅ Library search returns real npm/PyPI results
- ✅ Caching works with Serena
- ✅ Results parse correctly

**Gate 3** (after Wave 3):
- ✅ Can execute simple task end-to-end
- ✅ Validation runs real tests
- ✅ Git creates real commits
- ✅ E2E test passes

**Gate 4** (after Wave 4):
- ✅ Works on 3 project types
- ✅ Documentation is honest
- ✅ No "preview" markers
- ✅ Production ready

---

## Parallel Execution Strategy

**Wave 1** (3 agents in parallel):
- Agent A: ValidationOrchestrator (45 min)
- Agent B: GitManager (30 min)
- Agent C: Dead code removal (30 min)
- **Total Wall Time**: 45 minutes (parallel) vs 1h 45m (sequential)

**Wave 2** (3 agents in parallel):
- Agent A: Web search (1.5 hours)
- Agent B: Result parsing (1 hour)
- Agent C: Serena caching (30 min)
- **Total Wall Time**: 1.5 hours (parallel) vs 3 hours (sequential)

**Wave 3** (3 agents in parallel):
- Agent A: Simple executor (2 hours)
- Agent B: CLI integration (30 min)
- Agent C: E2E test (1.5 hours)
- **Total Wall Time**: 2 hours (parallel) vs 4 hours (sequential)

**Wave 4** (1 agent):
- Final polish (1-2 hours)

**Total Wall Time with Parallelization**: 5-6 hours  
**Total Sequential Time**: 10-12 hours  
**Efficiency Gain**: 50% faster

---

## Success Criteria

V3.5 is COMPLETE when:

✅ Can run: `shannon exec "add feature"`  
✅ Actually discovers libraries from npm/PyPI  
✅ Actually runs tests (pytest, npm test, etc.)  
✅ Actually creates git branch  
✅ Actually commits if validated  
✅ Works on Python, React, and iOS projects  
✅ E2E test proves it works  
✅ No stubs, no TODOs, no "preview mode"  

---

## Execution Plan

Starting NOW:
1. Wave 1: Remove stubs (45 min parallel)
2. Wave 2: Real library search (1.5 hours parallel)
3. Wave 3: End-to-end orchestration (2 hours parallel)
4. Wave 4: Production polish (1-2 hours)

**Total**: 5-6 hours wall time with parallel execution

Let's begin.

